# app.py
# Streamlit webapp to:
# 1) Upload PDF(s)
# 2) Split into chunks
# 3) Generate succinct per‑chunk context with Ollama (with live debugging)
# 4) Review & download JSONL
# 5) Push to Postgres PGVectorStore with a custom table name

import os
import io
import json
import time
import hashlib
from datetime import datetime
from collections import defaultdict
from typing import List, Dict, Tuple

import streamlit as st

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

from langchain_ollama import OllamaEmbeddings
import ollama

from langchain_postgres import PGEngine, PGVectorStore

# -----------------------------
# Sidebar Config
# -----------------------------
st.set_page_config(page_title="PDF → Contextual Chunks → PGVector", layout="wide")

with st.sidebar:
    st.title("⚙️ Settings")

    # Chunking
    CHUNK_SIZE = st.number_input("Chunk size", min_value=200, max_value=4000, value=1000, step=50)
    CHUNK_OVERLAP = st.number_input("Chunk overlap", min_value=0, max_value=1000, value=200, step=10)

    # Ollama
    OLLAMA_BASE_URL = st.text_input("Ollama Base URL", value="http://ollama123:11434")
    OLLAMA_MODEL = st.text_input("LLM for context (chat)", value="mistral")
    OLLAMA_EMBED_MODEL = st.text_input("Embedding model", value="nomic-embed-text")

    # Postgres
    PG_CONN = st.text_input(
        "Postgres (psycopg3) connection string",
        value="postgresql+psycopg3://myuser:mysecret@localhost:5432/mydatabase",
    )
    TABLE_NAME = st.text_input("Custom table name (new dataset)", value=f"pdf_context_{datetime.now().strftime('%Y%m%d_%H%M')}" )

    VECTOR_SIZE = st.number_input("Vector size (embedding dims)", min_value=128, max_value=4096, value=768, step=1,
                                  help="For nomic-embed-text this is 768.")

    st.markdown("---")
    st.caption("Tip: Make sure your Ollama daemon can reach the selected model(s).")

# Apply Ollama base URL
ollama.base_url = OLLAMA_BASE_URL

# ---------------------------------
# Helpers
# ---------------------------------
if "processed" not in st.session_state:
    st.session_state.processed: List[Dict] = []  # list of {id, text, metadata, prompt, response}
if "latest_debug" not in st.session_state:
    st.session_state.latest_debug = {"prompt": "", "response": "", "meta": {}}


def build_prompt(whole_doc: str, chunk_text: str) -> str:
    return f"""
<document>
{whole_doc}
</document>

Here is the chunk we want to situate within the whole document:
<chunk>
{chunk_text}
</chunk>

Please give a short, succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk
Answer in French, ONLY with the succinct context and nothing else.
""".strip()


def get_context_from_ollama(whole_doc: str, chunk_text: str, model: str, max_attempts: int = 5, step_ratio: float = 0.8) -> Tuple[str, str]:
    trimmed = whole_doc
    for attempt in range(max_attempts + 1):
        prompt = build_prompt(trimmed, chunk_text)
        try:
            response = ollama.chat(model=model, messages=[{"role": "user", "content": prompt}])
            result = response["message"]["content"].strip()
            return result, prompt
        except Exception as e:
            if attempt == max_attempts:
                return "", prompt
            # shrink context and retry
            trimmed = trimmed[: int(len(trimmed) * step_ratio)] if trimmed else ""
            time.sleep(0.1)
    return "", prompt


def hash_chunk_id(src_path: str, page: int, start_index: int, content: str) -> str:
    h = hashlib.sha256()
    h.update(str(src_path).encode("utf-8"))
    h.update(str(page).encode("utf-8"))
    h.update(str(start_index).encode("utf-8"))
    h.update(content[:200].encode("utf-8"))
    return h.hexdigest()[:16]


# ---------------------------------
# Upload & Process
# ---------------------------------
st.title("📚 PDF → Contextual Chunks → PGVector")
st.write("Upload PDF files, generate succinct contextual chunks with Ollama, inspect prompts/responses, then push to Postgres with a custom table name.")

uploads = st.file_uploader("Drop one or more PDFs", type=["pdf"], accept_multiple_files=True)

col_run, col_reset, col_download = st.columns([1,1,1])

if col_reset.button("Reset session"):
    st.session_state.processed = []
    st.session_state.latest_debug = {"prompt": "", "response": "", "meta": {}}
    st.experimental_rerun()

# ----------
# PROCESS
# ----------
if col_run.button("Run contextualization", type="primary", disabled=not uploads):
    st.session_state.processed = []

    # Save uploads to temp files so PyPDFLoader can read paths
    temp_paths: List[str] = []
    with st.status("Saving uploads…", expanded=False) as status:
        for uf in uploads:
            data = uf.read()
            tmp_path = os.path.join(st.experimental_user_dir(), f"upload_{int(time.time()*1000)}_{uf.name}")
            with open(tmp_path, "wb") as out:
                out.write(data)
            temp_paths.append(tmp_path)
        status.update(label=f"Saved {len(temp_paths)} file(s)", state="complete")

    # Load pages
    docs: List[Document] = []
    with st.status("Loading PDFs…") as status:
        for p in temp_paths:
            try:
                loader = PyPDFLoader(p)
                docs.extend(loader.load())
            except Exception as e:
                st.warning(f"Error loading {p}: {e}")
        status.update(label=f"Loaded {len(docs)} pages", state="complete")

    # Split
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        add_start_index=True,
    )

    with st.status("Splitting into chunks…") as status:
        chunks = splitter.split_documents(docs)
        status.update(label=f"Created {len(chunks)} chunks", state="complete")

    # Group by (source, page) to build perpage full_doc context
    by_page = defaultdict(list)
    for d in chunks:
        by_page[(d.metadata.get("source"), d.metadata.get("page"))].append(d)
    for key in by_page:
        by_page[key].sort(key=lambda d: (d.metadata.get("start_index") or 0))

    # Contextualize chunks with live progress
    progress = st.progress(0.0, text="Calling Ollama…")
    total = sum(len(v) for v in by_page.values()) if by_page else 1
    done = 0

    for (src, page), page_chunks in by_page.items():
        full_doc = " ".join([c.page_content for c in page_chunks])
        for idx, d in enumerate(page_chunks):
            src_path = d.metadata.get("source", "") or src or ""
            file_name = os.path.basename(src_path) if src_path else "uploaded.pdf"
            start_index = d.metadata.get("start_index") or 0
            end_index = start_index + len(d.page_content)

            chunk_id = hash_chunk_id(src_path, page, start_index, d.page_content)

            # LLM call
            context_text, prompt_used = get_context_from_ollama(full_doc, d.page_content, model=OLLAMA_MODEL)
            contextualized_chunk = f"{context_text}\n\n{d.page_content}" if context_text else d.page_content

            meta = {
                "file_name": file_name,
                "chunk_index": idx,
                "page": page,
                "total_chunks_per_page": len(page_chunks),
                "start_index": start_index,
                "end_index": end_index,
                "ingested_at": datetime.now().isoformat(),
                "chunk_id": chunk_id,
                "source": src_path,
            }

            st.session_state.processed.append({
                "id": chunk_id,
                "text": contextualized_chunk,
                "metadata": meta,
                "prompt": prompt_used,
                "response": context_text,
            })

            # Update latest debug panel data
            st.session_state.latest_debug = {
                "prompt": prompt_used,
                "response": context_text,
                "meta": meta,
            }

            done += 1
            progress.progress(done/total, text=f"Processed {done}/{total} chunks…")
    progress.empty()

# -----------------------------
# Live Debug / Inspection
# -----------------------------
left, right = st.columns([1,1])

with left:
    st.subheader("Latest LLM Call (live)")
    with st.expander("📤 Prompt sent to LLM", expanded=True):
        st.code(st.session_state.latest_debug.get("prompt", ""), language="markdown")
    with st.expander("📥 Context received from LLM", expanded=True):
        st.code(st.session_state.latest_debug.get("response", ""), language="markdown")
    with st.expander("🧾 Chunk metadata", expanded=False):
        st.json(st.session_state.latest_debug.get("meta", {}))

with right:
    st.subheader("Inspect a specific chunk")
    ids = [row["id"] for row in st.session_state.processed]
    if ids:
        selected_id = st.selectbox("Chunk ID", options=ids)
        selected = next((r for r in st.session_state.processed if r["id"] == selected_id), None)
        if selected:
            st.markdown("**Contextualized text**")
            st.text_area("", selected["text"], height=240)
            with st.expander("Prompt used"):
                st.code(selected["prompt"], language="markdown")
            with st.expander("LLM response (context)"):
                st.code(selected["response"], language="markdown")
            with st.expander("Metadata"):
                st.json(selected["metadata"])
    else:
        st.info("No chunks yet — run contextualization above.")

# -----------------------------
# Download JSONL
# -----------------------------
if st.session_state.processed:
    jsonl_buf = io.StringIO()
    for row in st.session_state.processed:
        jsonl_buf.write(json.dumps({
            "id": row["id"],
            "text": row["text"],
            "metadata": row["metadata"],
        }, ensure_ascii=False) + "\n")
    col_download.download_button(
        label="Download contextual_chunks.jsonl",
        data=jsonl_buf.getvalue().encode("utf-8"),
        file_name=f"contextual_chunks_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl",
        mime="application/json",
    )

# -----------------------------
# Push to PGVector 
# -----------------------------
st.markdown("---")
st.subheader("Send to PGVectorStore (custom table)")

push = st.button("Send to Postgres (create table if needed)", type="primary", disabled=not st.session_state.processed)

if push:
    try:
        with st.status("Connecting to Postgres & creating vector table…", expanded=False) as status:
            engine = PGEngine.from_connection_string(url=PG_CONN)
            engine.init_vectorstore_table(table_name=TABLE_NAME, vector_size=VECTOR_SIZE)
            status.update(label=f"Table ready: {TABLE_NAME}", state="complete")

        with st.status("Preparing embedding service…", expanded=False) as status:
            embedder = OllamaEmbeddings(model=OLLAMA_EMBED_MODEL, base_url=OLLAMA_BASE_URL)
            store = PGVectorStore.create_sync(
                engine=engine,
                table_name=TABLE_NAME,
                embedding_service=embedder,
            )
            status.update(label=f"Embedding model ready: {OLLAMA_EMBED_MODEL}", state="complete")

        # Insert documents (per-chunk) — sequential for transparency; can be batched later
        errors = 0
        pbar = st.progress(0.0, text="Inserting…")
        for i, row in enumerate(st.session_state.processed, start=1):
            try:
                doc = Document(page_content=row["text"], metadata=row["metadata"]) 
                store.add_documents([doc], ids=[row["id"]])
            except Exception as e:
                errors += 1
                st.warning(f"Failed to insert {row['id']}: {e}")
            pbar.progress(i/len(st.session_state.processed), text=f"Inserted {i}/{len(st.session_state.processed)}…")
        pbar.empty()

        if errors == 0:
            st.success(f"✅ All {len(st.session_state.processed)} chunks inserted into table `{TABLE_NAME}`.")
        else:
            st.warning(f"Completed with {errors} error(s). Inserted into `{TABLE_NAME}`.")

    except Exception as ex:
        st.error(f"❌ Push failed: {ex}")

# -----------------------------
# Footer: Versions & Hints
# -----------------------------
st.markdown("---")
st.caption(
    "Dependencies: streamlit, langchain-community, langchain-text-splitters, langchain-ollama, langchain-postgres, ollama.\n"
    "Run: `streamlit run app.py`"
)
